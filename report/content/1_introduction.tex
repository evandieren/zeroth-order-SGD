\section{Introduction}

In many applications, one has to solve optimisation problems whose objective functions do not permit gradient computations \cite{alarie2021two}. As a result, gradient-free optimization methods that only evaluate the function are required to solve these black-box problems. Zeroth Order Stochastic Gradient Descent (ZO-SGD) is a class of optimisation algorithms that use difference methods to approximate the gradient. The performance of ZO-SGD depends on a variety of hyperparameters and the difference method used. We investigate these dependencies, also explored in \cite{liu2020primer} for black-box deep learning, on simple multilayer perceptrons, putting our focus on the effect of dimensionality on performance and computation time.

% Therefore, researchers had to develop gradient-free optimization methods that could optimize those non-differentiable complex objective functions by only using their evaluations. 

% Zeroth order methods are derivative-free optimization methods which do not require gradient computation. Compared to first-order methods, those methods rely on gradient approximations such as Finite Difference, or any other approximation method which only uses function evaluations to determine the optimal solution. Therefore, those methods enable one to optimize non-differentiable functions. However, computing gradient approximations might become computationally expensive due to numerous function evaluations, but are sometimes the only remaining tool when other optimization methods fail to be applied for a given non-differentiable problem.

% In particular, Section \ref{subsec:ZO-SGD} describes Zeroth order Stochastic Gradient Descent (ZO-SGD), which is an extension for non-differentiable functions of the original SGD algorithm. In Section \ref{sec:exp}, we apply the ZO-SGD method to the Iris dataset. Lastly, Section \ref{sec:conclusion} concludes with take-home messages and the next steps regarding this project.
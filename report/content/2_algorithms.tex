\section{Algorithms}

\paragraph{Stochastic Gradient Descent}
In many applications, objective functions have the form 
\begin{equation}\label{GeneralProb}
    f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x),
\end{equation}
where $f_i$ is the loss function of the $i$-th observation, and $n$ the number of observations. Stochastic Gradient Descent (SGD) is the preferred algorithm for optimising such functions, balancing low computational cost per step, requiring one $\nabla f_i$ to be computed, and a decent convergence rate of $\mathcal{O}\left(\frac{1}{\epsilon}\right)$ \cite{bottou2018optimization} (see Appendix~\ref{app:formal_algorithms} for the pseudo-code of the algorithm). However, SGD does require gradient computation, which is impossible if $f$ or individual $f_i$ are non-differentiable.
%\subsection{Stochastic Gradient Descent}\label{subsec:SGD}

% Gradient Descent (GD) may become very computationally expensive (see Appendix~\ref{app:formal_algorithms} for pseudo-code). Hence, it might become computationally expensive to compute its gradient (sum of $n$ individual gradients), which is required by the Gradient Descent (GD) algorithm (see Appendix~\ref{app:formal_algorithms} for pseudo-code). Therefore, SGD only computes the gradient of one randomly selected $f_i$ in each iteration, reducing the computational cost of each iteration by over a factor of $n$ (see Appendix~\ref{app:formal_algorithms} for the algorithm). As a result, SGD only has $\mathcal{O}\left(\frac{1}{\epsilon}\right)$ convergence \cite{bottou2018optimization}. To compare with the original GD algorithm, GD would be faster for small $n$, but when $n$ is large (as is the case in many modern machine learning settings), SGD converges faster \cite{TODO}. One can easily see that this method requires gradient computation, which might cause problems when the randomly selected $f_i$ is non-differentiable. Hence, we describe hereunder a Zeroth order version of SGD.

% However, the stochasticity of SGD's method introduces variance into the algorithm and this leads to a significant disadvantage. SGD can only converge to an $\mathcal{O}\left(\eta\right)$-radius ball centered around the local minimum $w^*$, where $\eta$ is the constant step-size \cite{bottou2018optimization}. If $\eta$ varies, either using a step-size scheduler or a line-search, then SGD can converge to the local minimum, but then the rate of convergence reduces to $\mathcal{O}\left(\frac{1}{\epsilon^2}\right)$ \cite{reddi2016stochastic} and so many of its benefits are lost.

%\label{subsec:ZO-SGD}
\paragraph{Zeroth Order Stochastic Gradient Descent}
Zeroth Order Stochastic Gradient Descent (ZO-SGD) is a class of optimisation methods that employ difference methods to approximate a stochastic gradient and then emulate SGD (see Appendix~\ref{app:formal_algorithms} for the pseudo-code of algorithms). There are two categories of difference methods, depending on the number of function evaluations needed: one-point and multi-point methods \cite{liu2018zerothorder}

The one-point estimate can be defined as
\begin{equation}
    \hat{\nabla} f(\bold{x}) = \frac{\phi(d)}{\mu}f(\bold{x}+\mu \bold{u})\bold{u},
\end{equation}
where $\bold{u} \sim P$ is a random direction vector which follows a given distribution $P$, $\phi$ denotes a dimension-linked factor related to the choice of $P$, and $\mu>0$ is a perturbation radius. Typically, $P := \mathcal{N}(\bold{0}_d,\bold{I}_{d\times d})$ or a uniform distribution over the unit sphere such that $P := U(\mathcal{S}(0,1))_{d}$. Based on the choice of $P$, we let $\phi(d)=1$ for $P = \mathcal{N}(\bold{0}_d,\bold{I}_{d\times d})$ and $\phi(d)=d$ for $P = U(\mathcal{S}(0,1))_{d}$.

%Furthermore, if one defines $f_\mu(\bold{x}) = \E_u[f(\bold{x}+\mu \bold{u})]$, one can check the unbiasedness of $\hat{\nabla} f(\bold{x})$ with respect to $\nabla f_\mu$, i.e. $\E_u[\hat{\nabla}f(\bold{x})] = \nabla f_\mu(\bold{x})$, thanks to \cite{berahas2021theoretical,Nesterov2017RandomGM}. However, even though it is unbiased with respect to the expectation of the perturbed objective function's gradient, it is not with respect to its true gradient.

%\paragraph{Multi-point estimate}
The two-point estimate extends the single-point estimate by substracting the evaluation of the objective function at $\bold{x}$, as seen below:
\begin{equation}
    \hat{\nabla} f(\bold{x}) := \frac{\phi(d)}{\mu}\left(f(\bold{x}+\mu \bold{u})-f(\bold{x})\right)\bold{u}
    \label{eq:2.3}
\end{equation}

%which also satisfies the unbiaseness by linearity of the expectation, as long as $\bold{u}$ has zero mean.

%One can now check the mean squared error between the approximation and the true value of the gradient, in order to find the error order \cite{berahas2021theoretical,liu2018zerothorder} :
%\begin{equation}\label{eqn:mse_mult}
%    \E[\| \hat{\nabla}f(\bold{x})- \nabla f(\bold{x}) \|^2_2] %= \|\nabla f(\bold{x})\|^2_2 \mathcal{O}(d) + \mathcal{O}\left(\mu^2\left[\frac{d^3 + d}{\phi(d)}\right]\right)
%\end{equation}
%From \ref{eqn:mse_mult}, one notes that as $\mu$ decreases, the second error term decreases, but one has to be careful when reducing $\mu$ as this might cause stability issues. An important point is that the variance is proportional to the dimension (see first term), and this error is irreducible. To tackle this, variance-reduction extensions of SGD has been created, but will not be discussed in this work.

Additionally, one can use the mini-batch technique, drawing $\{\bold{u}_i\}_{i=1,...,b} \sim P$, and then define the following two-point estimate
\begin{equation}
    \hat{\nabla} f(\bold{x}) := \frac{\phi(d)}{\mu}\sum_{i=1}^b\left(f(\bold{x}+\mu \bold{u}_i)-f(\bold{x})\right)\bold{u}_i
\end{equation}
%which lands the approximation error \cite{berahas2021theoretical}
%\begin{equation}\label{eqn:mse_mult_batch}
%    \E[\| \hat{\nabla}f(\bold{x})- \nabla f(\bold{x}) \|^2_2] = \|\nabla f(\bold{x})\|^2_2 \mathcal{O}(\frac{d}{b}) + \mathcal{O}\left(\frac{\mu^2d^3}{\phi(d)b}\right)+ \mathcal{O}\left(\frac{\mu^2d}{\phi(d)}\right).
%\end{equation}
When the number of function evaluations reaches $d$, one can decide to use coordinate-wise gradient estimate instead of randomly drawing $\bold{u}_i$'s. This yields the coordinate-wise two-point estimate
\begin{equation}
    \hat{\nabla} f(\bold{x}) := \frac{1}{\mu}\sum_{i=1}^d\left(f(\bold{x}+\mu \bold{e}_i)-f(\bold{x})\right)\bold{e}_i,
    \label{eq:2.5}
\end{equation}
where $\{\bold{e}_i\}_{i=1,...,d}$ is the canonical basis of $\R^d$. 

%This yields a lower estimation error of $\mathcal{O}(d\mu^2)$ \cite{berahas2021theoretical}.

% Regarding the ZO-SGD, it resembles its first-order   counterpart, except that the gradient is now computed using one of the previously described estimates, as seen in Appendix \ref{app:formal_algorithms}.
\section{Experiments}\label{sec:exp}
We now investigate the performance of ZO-SGD in an empirical setting, determining whether it is a viable class of optimisation algorithms. We conduct this investigation by assessing the impact of the difference method and hyperparameters $\mu$ and $d$ on the ZO-SGD's proficiency to train multilayer perceptrons (MLPs). Our code can be found on \href{https://github.com/EdmundHofflin/Optimisation_ML_Project}{Github}.

\subsection{Dataset Description}
% PENDIGITS DESCRIPTION - NOT RELEVANT ANYMORE:
% For a meaningful study of MLPs' learning, we trained our models on the pendigits dataset. This dataset of 10,992 instances comprises 17 numeric features describing the handwritten digits 0 to 9 \cite{pendigits}. The set therefore contains a total of 10 well-balanced classes: we verified that each of the classes has at least 1000 samples.\\
% We chose this dataset as it is sizable (the iris dataset, for example, was too small and simple, which led to too fast convergence), but also well suited for standard MLPs: adequate hyperparameter choices can reach accuracies of up to 99 $\%$. Most supervised ML datasets available online contain images, but they pose too much of a challenge for standard MLPs, and we preferred to avoid CNNs or Transformers to better understand the effect of dimensionality on convergence rates.

% IRIS DATASET
We performed our experiments on the Iris dataset, which is a small-size set well adapted for simple testing of multiclass classification. The dataset of 150 instances comprises 4 numeric features, is well-balanced and proved sufficiently complex to explore some interesting trends for the simple two-layer MLPs we studied. We also tested the bigger pendigits dataset (10 992 instances), but the computational cost of ZO-SGD made experiments impractical.

\subsection{Methodology}
We elected to train perceptrons with a single hidden layer, whose dimension we varied. We limited the tests to such simple models to ensure a direct relation between the dimensionality of the problem and the performance of ZO-SGD. We tested two standard activation functions, ReLU and sigmoid. However their results and trends were near identical, so we only present the ReLU results for clarity and to demonstrate ZO-SGD optimising non-differentiable functions. We used a learning rate of $0.01$, $100$ epochs, a batch-size of $32$, and the cross-entropy loss. Additionally, all models were initialised with the same seed.

We varied the dimension of the hidden layer over $\left\{3, 4, 8, 16, 32, 64\right\}$, the perturbation radius hyperparameter $\mu$ over $\left\{1, 10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\right\}$, and the batch number $b$ over $\left\{1, 10, 10^2, 10^3\right\}$. Finally, for all tests, we compared the multi-batch versions of one-point and two-point, and coordinate ZO-SGD against vanilla SGD. We will omit ``multi-batch" for clarity.

% For a comprehensive and scalable study of the effect of dimensionality on the convergence of our algorithm, we chose to train perceptrons with one hidden layer whose size we varied. \\
% We tested two activation functions for the hidden layer (both for SGD and ZO-SGD learning): ReLU and sigmoid, two standard functions used in the ML community. In both cases we opted for the cross entropy loss: it is a widely used loss for supervised ML problems, and it is non-convex, making the study of its learning dynamics particularly interesting.\\
% We trained our ReLU and sigmoid neural networks with both SGD and ZO-SGD, a learning rate of 0.01, 1000 epochs and a batch size of 64: a few preliminary tests showed that those learning parameters yielded good performance on average. We chose to keep them constant to give more validity to direct comparisons of learning methods and hyperparameters. As explained earlier, we compute SGD on small batches for variance-reduction purposes. \\
% The hyperparameters we did vary during training were: the number of hidden neurons (for the study of the effect of dimensionality on convergence), and the learning radius $\mu$ and N (WHAT IS THIS, DOES NOT APPEAR EARLIER DOES IT???) of the ZO-SGD algorithm.

\subsection{Results}

\paragraph{Batch requirement of one- and two-point methods}
The most immediate result was the high batch $b$ requirement of both the one- and two-point estimation methods. The one-point method was never stable and did not consistently converge, even with $b = 1000$. The two-point method was somewhat stable, converging in roughly $50\%$ of tests with $b = 1000$, but very infrequently with $b = 100$ and never with $b = 1$ or $10$. These results align with the theory: one-point estimate suffers from extremely high variance, slowing or preventing convergence, hence multi-point estimates are almost exclusively used in practice \cite{flaxman2004online}. Given this, we focus our attention hereafter on the two-point estimation method with $b = 1000$ and the coordinate estimation method.

\paragraph{General comparison of two-point and coordinate ZO-SGD and SGD}
Figure~\ref{fig:plot_1} compares these methods against SGD for $\mu = 0.01$ and $d = 4$. We observe that the SGD  has a lower loss than its two other zeroth-order methods. The two-point and coordinate ZO-SGD have similar behaviours, and do not have a high loss reduction over the 100 epochs. Furthermore, the computational time in seconds per epoch was approximately $1$, $0.1$ and $0.001$, for two-point, coordinate, and SGD, respectively. So not only did ZO-SGD converge more slowly than SGD, it required orders of magnitude more computation. While two-point and coordinate estimation variants of ZO-SGD are valid optimization methods, they are significantly slower than SGD, and should therefore be used when only necessary (i.e. when SGD is not available).

\paragraph{Impact of Dimensionality}
% Computational cost linear in d. Much higher than SGD
% Instability of two-point at higher dimensions
% Discuss wrt # hidden layers (and link to high dimensionality)

Figure~\ref{fig:plot_2} shows the results of our study of the effect of dimensionality (varied by changing the number of hidden neurons of our 2-layer network) on convergence and computation time. The left plot of  Figure~\ref{fig:plot_2} shows that SGD and coordinate descent are quite robust methods: they remained stable for the range of dimensions tested. However, the loss of two-point estimates depends heavily on the number of hidden layers, suggesting a stability issue. This instability is not surprising when we know from \cite{liu2020primer} that the variance of mini-batch stochastic gradients for this method is
\begin{equation}\label{eqn:mse_mult_batch}
   \E[\| \hat{\nabla}f(\bold{x})- \nabla f(\bold{x}) \|^2_2] = \|\nabla f(\bold{x})\|^2_2 \mathcal{O}(\frac{d}{b}) + \mathcal{O}\left(\frac{\mu^2d^3}{\phi(d)b}\right)+ \mathcal{O}\left(\frac{\mu^2d}{\phi(d)}\right).
\end{equation}
We can see in this equation that the variance scales with $d^3$, which could explain the instability with increasing dimensions. Coordinate ZO-SGD does not suffer from that problem: the same variance scales with $\mathcal{O}\left(d\mu^2\right)$ for this method.

Figure \ref{fig:plot_2} also shows that both the computation time of the two-point estimate and coordinate descent scale linearly with the dimensionality. This is consistent with theory: we can see from equations \eqref{eq:2.3} and \eqref{eq:2.5} that the complexities of the gradient approximations are respectively of $\mathcal{O}\left(b d\right)$, as we sample $u_i$ from $\R^d$ and $\mathcal{O}\left(d\right)$. SGD theoretically also has a complexity of $\mathcal{O}\left(d\right)$, but that was hardly visible in the experimental plot given its small slope.

\paragraph{Impact of $\mu$}
Figure~\ref{fig:plot_3} shows the relationship between the perturbation radius $\mu$ and the performance of two-point and coordinate estimation methods for ZO-SGD, varying $\mu$ over $\left\{10^{-1}, 10^{-3}, 10^{-5}\right\}$. As evidenced by Equation~\eqref{eqn:mse_mult_batch}, the variance of these difference methods scales with $\mu^2$. However, in Figure~\ref{fig:plot_3}, the coordinate method is seemingly invariant under the changes in $\mu$, and the two-point method only alters its loss trajectory with minimal, if any, change in variance. We suspect that this minimal impact is due to the relative scale of $\mu$: it is several orders of magnitude smaller than $d$, the most significant variance factor, and is potentially even small enough to be comparable to numerical errors. Therefore, at this scale we conclude that $\mu$ is a less significant hyperparameter than $d$ for the performance of ZO-SGD methods.